{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import operator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to save and load file to reduce processing time.\n",
    "import pickle\n",
    "def save(word_list, name):\n",
    "    with open(name, 'wb') as fp:\n",
    "        pickle.dump(word_list, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# func to load:\n",
    "def load(name):\n",
    "    with open(name, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Loading files \n",
    "\n",
    "# 'List' or 'target' will contain the output of our data.\n",
    "# path1 is the path in which required files are stored. \n",
    "lst = []\n",
    "path1 = 'C:\\\\Users\\\\Archit\\\\Desktop\\\\new\\\\20_newsgroups_train'\n",
    "for root, dirs, files in os.walk(path1):\n",
    "    for name in dirs:\n",
    "        lst.append(name)\n",
    "target = lst\n",
    "print(len(target))\n",
    "\n",
    "# loading stop_words and punctuations:\n",
    "# We have to remove stop_words and punctuations from email to reduce the unneccessary words.\n",
    "stop_words = []\n",
    "corpus_path = 'C:\\\\Users\\\\Archit\\\\Desktop\\\\new\\\\stopwords_en.txt'  # path where stop_words are stored.\n",
    "review_file = open(corpus_path,\"r+\")#.read()\n",
    "    #print(review_file)\n",
    "for line in review_file:\n",
    "    stop_words+=review_file.readlines()\n",
    "    \n",
    "# 'punct' will contain punctuations. \n",
    "punct = string.punctuation\n",
    "punct = list(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If stop_words contains '\\n' then, UNCOMMENT this cell to remove '\\n' from all stop_words.\n",
    "# stop_words = [ w.replace(\"\\n\", \"\") for w in stop_words]\n",
    "# print(stop_words,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296275"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, all_words will contain all the words from each file.\n",
    "# this may take a while.\n",
    "all_words = []\n",
    "for name in lst:    # loop over each folder.\n",
    "    path = \"C:\\\\Users\\\\Archit\\\\Desktop\\\\new\\\\20_newsgroups_train\\\\\" + name   # path now becomes path_folder + name of folder.\n",
    "    for file in glob.glob(os.path.join(path, '*')):\n",
    "        file_o = open(file,'r').read()                                 \n",
    "        #print(file_o)\n",
    "        words = file_o.split()\n",
    "        word_len = len(words)\n",
    "        for w in words:                              \n",
    "            if w!='Date:' or w!='date:':        # removing header when 'date' is encountered.\n",
    "                words.remove(w)\n",
    "            elif w=='Date:' or w=='date:':\n",
    "                break\n",
    "        k = 0.03*(word_len)                    # removing starting 3% words from each file after 'date' encountered.\n",
    "        k=int(k)\n",
    "        words = words[k:]                                                   # removing header.\n",
    "        words = [x.lower() for x in words]                                   # converting to lower_case.\n",
    "        all_words += words\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217764"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dictionary of all the words with key as word_name and count of that word as value.\n",
    "from collections import Counter      # inbuilt function.\n",
    "dictionary = Counter(all_words)      # dictionary will store (word_name, word_cnt) as key and value resp.\n",
    "key = dictionary.keys()\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217764"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting dictionary in descending order based on values.\n",
    "sorted_dict = sorted(dictionary.items(), key=operator.itemgetter(1), reverse=True)\n",
    "len(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_dict[:500],end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183042 "
     ]
    }
   ],
   "source": [
    "# Initialising features as NULL. \n",
    "# Iterate over each word in sorted dictionary (descending order) and check if it's not an stop_word or string_punc then add this word in feature_list.\n",
    "features=[]\n",
    "for name,cnt in sorted_dict:\n",
    "    if name not in stop_words and name not in punct and name.isdigit()==False and len(name)>1 and len(name)<=20:     # for cleaning purpose.\n",
    "        features.append(name)\n",
    "features = features[30:]                 # removing topmost 30 words(as they're too common so they're of no use).\n",
    "print(len(features),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 452\n",
      "183042 182531\n",
      "182531 147944 131335 131246\n"
     ]
    }
   ],
   "source": [
    "# cleaning the features_list to remove unwanted words.\n",
    "\n",
    "remove_list_1 = ['called','__________________|always','whatever)','us).','________________________________________________________________________________steven', 'o---------------------------+======================================o','ace||','bs>','at?','return;', 'xfree86?', 'perror(\"\");', 'please...', '3do','com2','dc,','from:','files?', 'ave.,', 'sandvik@newton.apple.com', 'x.b','hmmm,',  \"doesn't,\", 'b.c.,', 'sandvik)','like?', 'des?','halat@bear.com', 'asala/sdpa/arf', 'to/from',  'rider)','usa)',\"isn't,\",  'a&m', 'ny,', 'game?', 'well...','to/from', 'eggs', 'anyway).', 'lynn', 'er,', 'wave/qwk', 'des?', 'ch>', 'roussel]','po5.andrew.cmu.edu', 'ed.,','rm.', 'dc,', 'eli', 'space?', 'bontchev@fbihh.informatik.uni-hamburg.de','hp-ux', 'names.', 'machine)', 'r&d',\"doesn't,\", 'halat@bear.com', 'sandvik@newton.apple.com', 'sandvik)', 'congress...\"', 'ed.green@east.sun.com', 'her!\"', 'ppm', 'thf2@kimbark.uchicago.edu', 'jpeg,', 'post)',  'number?', 'tk>',\"isn't,\",'children?',  'snichols@adobe.com',  'horne)', 'fred.mccall@dseg.ti.com',  'nntp@acsu.buffalo.edu',  'z1dan@exnet.iastate.edu', 'cb360t',  'mrs.', 'comp.windows.x',  'too).', 'luck!', 'kmr4@po.cwru.edu', 'ryan)',   'xt,', 'hmmm...','get?',\"other's\",'on...', 'rll',  '50mhz', 'uupcb', 'amd','king)', 'bj-200', 'ie.', 'quote:', 'group)',  'obo.','is).', 'them)', 'again?', 'jmd@handheld.com', 'atf/fbi', 'viking@iastate.edu', 'postmaster@ozonehole.com', 'nut', 'iici', 'thu', 'below,',  'wont',  'line?', 'eczcaw@mips.nott.ac.uk', 'werth!',  'hence,', 'site,', 'whatsoever', 'chip?', '512k','bible?', 'nt,', 'ssto', 'at,','correct?', 'js>', 'ps/2', 'time.\"', 'available?','he?', 'baalke@kelvin.jpl.nasa.gov', 'jb>', 'cover)','had.','geb@cs.pitt.edu', 'nsmca@acad3.alaska.edu', 'jacked', 'kratz', 'uh,', 'here)','w4wg','baalke@kelvin.jpl.nasa.gov', 'deleted...]', 'name)', 'question...', 'pm)','call', 'too)', \"ol'\", 'etc.?', 'that).','is!', 'issue?', 'anything?', 'time)', \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\",  'time...', 'signature-----', 'time...','call', 'ok.', 'e.g.', 'can.','again...', 'me).','else?', 'me;', 'great','we?','more)', 'do)', 'in)','it.)', 'them.\"','me)', 'are?','is)','cars?','yalcin]','them).', 'arms,-never--never--never!\"', 'one).','it),','etc?','myrto)','osf/motif',  'only)','osf/motif', 'not)', 'life?','no?', 'all)','example)', 'card)',  '_______',  'say?','______','__________________________________________________________________________','use?','etc.),', '___/', 'tcp/ip', 'masses', 'tt', 'am.','x{','well?','they?','st.,', '_is_', 'no:', 'etc.).','___|','info?', '19,', '1/4', '1/4','26,','17,', 'pa146008@utkvm1.utk.edu', 'but...', 'pa146008@utkvm1.utk.edu',  'low.', 'hard.', '1993:', '______________________________________________________________________________', 'then?', 'above)','others)', 'above)','of)','us?','out?','question','x}','home:', 'lives,', 'it).','ca>', '_______________________________________________________________________________', 'ones,','from?', 'ed.', 'for?',\"sun's\", 'al.', 'gnu','saves)','_/_/_/_/',  '_/_/','jr.', 'copies)','tel#:', 'ii,', 'gl','road.','all?','fprintf(stderr,', 'check_io(output,','real','waco,',  'yours.', '________________________________________________________________________________','found','drive','key','ideas?', 'them.','--','case','makes','year','free','etc.','kind','etc.','true','told','me.','man','full','time.','that.','now,','me,','(the','this.','you.','you,','that,','this,','them,','\"i','no,','here.','not.','all.','fax:','one.','up.','say,','sun','all,','there.','too.','out.','>that','(which','year.','know,','now.','(as','do.', 'and,','>this','(if', 'to:','_____','tel:','os/2','days.','etc.)','them?','own.','in,',\"bd's\", 'will.','n.','why?','used.','be,', 'first.','n.','down.','b)','not?','k.',\"he'll\",'above,','x11','a)','or,', 'man,', 'g.','for.','done.','_the','t.','at&t','do?','men,','lines:','p.s.', 'deleted]','x11r5','ed>','bad,','it.\"','was.','thanks!', 'of,', 'will,', 'etc.,', 'amp', 'w/','it)','be?','end,','pp.','here?', \"else's\",'p,#','there?', 'it!',  'i.', 'huh?','pp,', 'n3jxp','etc,', 'problem?','lines.', 'go.','see.','__/', 'm\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(','mind,','eh?', '____', 'lot.','ones.', 'far.','one?', 'die.','dod#','ra>','on?','_/_/_/', 'ozone', 'cnn','phi', 'geb@cadre.dsl.pitt.edu', 'so?','appears)','his/her', 'exists.', 'think)','x,',\"year's\",'eof_not_ok);','m/s','bds', 'm/s','god?', 'it...', 'him?', 'false.', 'public.', 'etc...','at.', 'odwyer@sse.ie', 'x/*', 'xfree86','etc)','_not_', 'to?',]\n",
    "remove_list_2 = ['auth_cnt);', 'more_auths', 'oname)',  'cats............',\"yorn[0]=='n'))\", 'printf(\"\\\\nplease', 'eof.', 'length;', 'eatchar', 'filler', 'val', 'fputc(uuencode(val),', 'failed\"', 'ioccc.', 'busy,', 'obfuscate.info', 'x11r4,', 'isc,',  '--', \"that's\",'apr','gmt','hm.', 'ks', 'them.)','pe-||','full....',  'janet:', 'rome,', 'ok?','huh.','article-i.d.:','you!\"', 'everyone', '28', 'works','nntp-posting-host:' ,'start',  '25', 'mon,','following','17', '23', 'thu,',  'fri,','tue,','gmt', 'enough','christian@aramis.rutgers.edu','distribution:', 'reply-to:', 'thu','certain','date:', 'message-id:', 'sender:', '21', 'organization:', 'references:', 'nntp-posting-host:''known', 'write', 'consider','april','26','15', 'understand','30', 'full',  'ask', 'pay', 'feel',  'went', 'mail','found','13','simply',   'says', '27', 'kind','send', 'far', 'whether', 'quite', 'work', 'article','several','32','gets','>this','up.', 'them,', \"what's\",'$100','-->','2)','saw', 'this?', '54','6)',  '(415)', '--if', 'it...', \"it'll\", '81','whoever', '>well,', '>who', '>>>>','8,', '20,','hopefully', '44',\"'92\",  '>>the','72',\"'91\",'it)', 'thoughts', '>which',   '---------','$1', 'away','2.','3.', 'so.', 'mentioned', '(as', 'j.','#>','whose', 'him.','14','11','took','supposed','too.', 'p.','say,', 'too.', 'p.', '12','24','cause','20','within','similar', 'instead', 'via',  'everything','often','well,', 'that.', 'lines:', 'me,','though','now,','making',  'came', 'perhaps', 'comes',  'means', 'based',  'heard','ever','seen','given', 'seem', 'always', 'three', 'maybe', 'less','next', 'new','nothing', 'great','>were', 'you!', '>>for', '(c)', 'thanx','=============================================================================','however.' 'article','else', 'available','copy', 'that,',  'makes','use.', '(--)',  'yes.','\\\\/', 'k',  '29', '1st','------------------------------------------------------------------------------',  'do.', 'knew', '>if',':)','>it', 'yet', 'remember', 'trying','93', '4.','16', '9', 'db','>as', '__', 'yes',  '>--','\"i','said,', 'clear', 'difference', 'asked', 'especially', 'four', 'upon','lot', 'able', 'keep','least','let',  'long', 'tell','help','look', 'problem', 'however,', 'best', 'thing', 'rather', 'first', 'since', 'really', 'believe', 'anyone', 'right', 'something','bd','people','much','part','every', 'little', 'used', 'need', 'good','no.','***', 'on,','//', '80','----------','(a)','58', 'u.', 'co', '>>is', 'cdt@vos.stratus.com', '*****','--------------------------------------------------------------------------', '73', '_/_/_/',  'm\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(','>their','*******************************************************************************', 'pa', '(the',  '_/','/*', \"there's\",\"they're\", 'also,', 'gave','>that', '>a', 'cut', 'not,', '1)','>what','---------------------------------------------------------------------', 'bbs', 'w.', '3)', '>>in', 'oh', 'm.', 'joe', 'became', '18','still', 'around', 'it,','although','actually','them.','almost', 'show', 'agree', 'certainly','anything', 'try','give', 'put', 'seems', 'probably','might','would','made', 'well', 'got', 'another', 'better', 'number', 'read', 'come', 'sure', 'things', 'someone','never', 'please', 'back', 'last', 'without','going', 'find', 'using','64','*not*',\"he's\",'want','way', 'say','>of','it?', '??','to.','----------------------------------------------------------------------------','-------------------------------------------------------------------------------', 'hp', 'lets', '55', '$3','etc.','|>|>', 'fine,', '\"i\\'m','>and','you.', 'you.','saying','like','8', 'whole', '6', 'told','make', 'is,','many','either', '1993','go', 'us','see','said', 'take','q','???','>:',  '>>>','six', 'r','>is','19', '22', '55.0', 'dr.', 'all,', '**','||', '(in','----','----------------------------------------------------------------------','$50','>|>',\"i'll\",  'you,', '>to','end', 'this.', '*/','(or',  '(i','\"the','big', 'evidence', 'getting', 'day', 'thought', 'car', 'done', 'cannot', 'high', 'order', 'person', '10', 'name', '\"the''----------------------------------------------------------------------','!=','--------------------------------------------------------------------------------', '7', '---',':-)','(and',':-)', ':-)',':-)' 'is,', 'one', '|>','5', '...', '>in','must', \"i've\",'>the','4', '>i',\"i'd\",  '3', \"can't\",'get', '>>', 'know', 'x','two', '0', 'it.', 'time', '2', \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\", 'think', 'writes:', 'use', 'also', 'could', 'even', '1', 'may', \"i'm\",]\n",
    "# print(len(remove_list_1), len(remove_list_2))\n",
    "\n",
    "# importing regex library to apply regex in features.\n",
    "import re \n",
    "print(len(features),end=\" \")\n",
    "for x in features:\n",
    "    if x in remove_list_1 or x in remove_list_2:\n",
    "        features.remove(x)\n",
    "print(len(features))\n",
    "# applying some regex\n",
    "\n",
    "exp = re.compile('\\W')\n",
    "exp2 = re.compile('\\d*\\W')\n",
    "exp3 = re.compile('^[\\W_]+$')\n",
    "\n",
    "print(len(features),end=\" \")\n",
    "features = [x for x in features if not exp.match(x)]\n",
    "\n",
    "print(len(features),end=\" \")\n",
    "features = [x for x in features if not exp2.match(x)]\n",
    "\n",
    "print(len(features),end=\" \")\n",
    "features = [x for x in features if not exp3.match(x)]\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the list of all words(stored in desc order of their count) which can be made as an feature.\n",
    "print(features,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129479"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying regex to remove digits.\n",
    "exp = re.compile('\\d')\n",
    "features = [x for x in features if not exp.match(x)]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking features from 30 to 7000.(top_freq word).\n",
    "# new_features is the actual words i'm using in this code as features.\n",
    "\n",
    "new_features = features[15:700]\n",
    "print(len(new_features))\n",
    "print(new_features,end=\"\")    # list of words i'm using as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now below is the code for self implementing Multinomial Naive Bayes and I've also done the predictions using Sklearn MultiNomialNB for comparing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit func will fit our data into dictionary according to target values.\n",
    "                                        #(structure of dict.): \n",
    "                             \n",
    "                             # dictionary['target_name']['feature_name']['count_feature'].\n",
    "            \n",
    "    \n",
    "def fit(training_path, target, features):        # defining fit func.\n",
    "    new_dictionary = {}                          # initialising dictionary\n",
    "    \n",
    "    for name in target:               # iteratng over each target values.\n",
    "        path = training_path + name   \n",
    "    #dic[name]['total_count']=0\n",
    "        val = 0                        # initialising temp variables.\n",
    "        ct = 0\n",
    "        temp_arr = np.zeros((len(features),1))             # this numpy_array will store count of each features in particular folder.\n",
    "        \n",
    "        for file in glob.glob(os.path.join(path, '*')):    # iterating over each file in target value.\n",
    "            file_o = open(file,'r').read()                 # for removing header.\n",
    "            new_list = file_o.split()\n",
    "            i=0\n",
    "            while new_list[i]!='Date' and new_list[i]!='date' and new_list[i]!='Date:':\n",
    "                i+=1\n",
    "            k_cnt = 0.03*(len(new_list))                    # removing starting 3% words from each file after 'date' encountered.\n",
    "            k_cnt = int(k_cnt)\n",
    "            new_list = new_list[k+i-8:]\n",
    "            new_list = [x.lower() for x in new_list]       # changing all words into lower_case.\n",
    "            new_dictionary[name] = {}                      # initialising dict with that particular target value.\n",
    "            for i in range(len(features)):\n",
    "                temp_arr[i][0] += new_list.count(features[i])    # finding count of each feature in each file of target folder.\n",
    "            val+=len(new_list)             \n",
    "            ct+=1                                           # ct will store count of each file present in particular target folder. \n",
    "            \n",
    "        # storing count of each features in each folder in dict.\n",
    "        for i in range(len(features)):\n",
    "            new_dictionary[name][features[i]] = temp_arr[i][0]\n",
    "            \n",
    "        # 'val' will contains count of all_words present in particular target folder.\n",
    "        new_dictionary[name][\"count_words_inFolder\"] = val         # total count of words in particular target folder\n",
    "        new_dictionary[name]['count_currClass'] = ct               # count of all files in particular folder i.e output m kitni bar woh target aara ha.\n",
    "   \n",
    "    tot_cnt = 0              # this will store all the counts of all target values i.e len(output).\n",
    "    for name in target:\n",
    "        tot_cnt += new_dictionary[name]['count_currClass']\n",
    "    new_dictionary['all_count'] = tot_cnt\n",
    "    \n",
    "    return new_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will cal the probability required to predict the class.\n",
    "\n",
    "def cal_probability(new_list, dictionary, curr_class):\n",
    "    output = 0                              # initialising output as 0 (using log)\n",
    "    features = dictionary[curr_class].keys()\n",
    "    features = list(features)                       # extracting total features.\n",
    "    for i in range(len(features)):                  # loop over all possible features.\n",
    "        if(features[i] == 'count_currClass' or features[i] == 'count_words_inFolder'): # some extra variables other then features, so continue.\n",
    "            continue\n",
    "            \n",
    "        count_part_feature = new_list.count(features[i])             # counting the occurence of a particular feature in test file.\n",
    "        count_feature_dict = dictionary[curr_class][features[i]] + 1    # occurence of part. feature in from dictionary\n",
    "        count_words_currClass = dictionary[curr_class]['count_words_inFolder'] + (len(features)-2)   # total words in current_class.\n",
    "        \n",
    "        # p will store: (probability that a particular feature belongs to curr_class) ** (occurence of that feature in test_file).\n",
    "        p = ((np.log(count_feature_dict) - np.log(count_words_currClass)) * count_part_feature)\n",
    "        output = output + p\n",
    "    \n",
    "    #print(\"prob:\", output)\n",
    "    count_class = dictionary[curr_class]['count_currClass']  # count of that particular class.(cnt of part. output)\n",
    "    total_count = dictionary['all_count']                    # total class count.(total output)\n",
    "    class_prob = np.log(count_class) - np.log(total_count)   # prob. of particular class.\n",
    "    output = output + class_prob\n",
    "    \n",
    "    return output                          # return calculated prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict for a single_file.\n",
    "def  Predict_for_singleFile(new_list, dictionary):\n",
    "    classes = dictionary.keys()        # extracting all possile outputs.\n",
    "    max_prob = -1                      # init. max_prob as -1\n",
    "    first_iter = False\n",
    "    prob = 0                           # this will give curr_calculated_prob.\n",
    "    best_class = -1\n",
    "    for curr_class in classes:         # for outpur in all poss_output:\n",
    "        if(curr_class == 'all_count'):\n",
    "            continue\n",
    "        prob = cal_probability(new_list, dictionary, curr_class)     # cal_prob for a single file belong to curr_class\n",
    "        #print(prob)\n",
    "        if(prob > max_prob or first_iter==False):                    # if calculated_prob > max_prob thn update it and update best_class.\n",
    "            max_prob = prob\n",
    "            best_class = curr_class\n",
    "            first_iter = True\n",
    "    return best_class                    # return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict output.\n",
    "def predict(path_x_test, folder_name_list, dictionary):\n",
    "    y_pred = []    # init. y_pred as NULL\n",
    "    temp = []\n",
    "    for name in folder_name_list:                   # for output in all poss_output.\n",
    "        #print(\"name:\", name)\n",
    "        path = path_x_test + name \n",
    "        cnt1=0\n",
    "        cnt2=0\n",
    "        #temp_dict[name]={}\n",
    "        for file in glob.glob(os.path.join(path, '*')):    # for each file in particular folder.\n",
    "            file_o = open(file,'r').read()[400:]\n",
    "            new_list = file_o.split()\n",
    "            output = Predict_for_singleFile(new_list, dictionary)         # finding best_class for that file.\n",
    "            if(cnt2%40 == 0):\n",
    "                print(output,end=\" \")\n",
    "            cnt2+=1\n",
    "            if(output==name):\n",
    "                cnt1+=1\n",
    "            y_pred.append(output)                          # adding the predicted best_class to y_pred.\n",
    "        print(\"count:\", cnt1)\n",
    "        temp.append((cnt1,cnt2))\n",
    "    return y_pred, temp               # return y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Function\n",
    "def get_score(prediction):\n",
    "    sum_p = 0                 # sum_p is predicted sum\n",
    "    sum_a = 0                 # sum_a is actual sum(sum of no of files in each folder.)\n",
    "    for p,a in prediction:    # 'p' is predicted count and 'a' is actual count\n",
    "        sum_p += p\n",
    "        sum_a += a\n",
    "    return ((sum_p/sum_a)*100)  # return accuracy.\n",
    "\n",
    "# print_outpt\n",
    "def prt_output(prediction):\n",
    "    for p,a in prediction:\n",
    "        print(\"Correctly Predicted:\", p, \"Total:\", a, \"Classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS TO RUN PREDICT FUNCTION.\n",
    "# set path where testing data is stored.\n",
    "\n",
    "# path_test = \"C:\\\\Users\\\\Archit\\\\Desktop\\\\new\\\\20_newsgroups_test\\\\\"\n",
    "# y_pred, temp = predict(path_test, target, new_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Predicted: 127 Total: 200 Classes\n",
      "Correctly Predicted: 141 Total: 200 Classes\n",
      "Correctly Predicted: 126 Total: 200 Classes\n",
      "Correctly Predicted: 143 Total: 200 Classes\n",
      "Correctly Predicted: 141 Total: 200 Classes\n",
      "Correctly Predicted: 132 Total: 200 Classes\n",
      "Correctly Predicted: 148 Total: 200 Classes\n",
      "Correctly Predicted: 167 Total: 200 Classes\n",
      "Correctly Predicted: 149 Total: 200 Classes\n",
      "Correctly Predicted: 166 Total: 200 Classes\n",
      "Correctly Predicted: 132 Total: 200 Classes\n",
      "Correctly Predicted: 160 Total: 200 Classes\n",
      "Correctly Predicted: 134 Total: 200 Classes\n",
      "Correctly Predicted: 145 Total: 200 Classes\n",
      "Correctly Predicted: 141 Total: 200 Classes\n",
      "Correctly Predicted: 152 Total: 200 Classes\n",
      "Correctly Predicted: 142 Total: 200 Classes\n",
      "Correctly Predicted: 142 Total: 200 Classes\n",
      "Correctly Predicted: 129 Total: 200 Classes\n",
      "Correctly Predicted: 93 Total: 200 Classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70.25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing number of correct_prediction and accuracy.\n",
    "prt_output(temp)\n",
    "get_score(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now below is the code for using sklearn implemented MultiNomialNB.\n",
    "First I've created the DataFrame to visualise the data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process starts for creating data_frame \n",
    "data_frame_features = new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6985"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_frame_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    # DATA_FRAME creation.(for training files.)\n",
    "# creating data-frame.\n",
    "k1=0        # init k1.\n",
    "data_frame_features.append('target_values_new')             # appending extra column in features to store the output.\n",
    "df_new = pd.DataFrame(columns = data_frame_features)        # creating an empty dataframe with columns as features and one extra column as output.\n",
    "\n",
    "for new_name in target:                       # iterating over each folder:\n",
    "    print(\"name:\", new_name)\n",
    "    #print()\n",
    "    path = \"C:\\\\Users\\\\Archit\\\\Desktop\\\\new\\\\20_newsgroups_train\\\\\" + new_name\n",
    "    for new_file in glob.glob(os.path.join(path, '*')):     # iterating over each file in that particular folder.\n",
    "       # print(\"file: \",new_file)\n",
    "        cnt_array = np.zeros((len(data_frame_features),1))   # creating an np array to store count of each 'feature' in each file.\n",
    "        file_n = open(new_file,'r').read()\n",
    "        new_words = file_n.split()                           # 'new_words' is a list that stores each word of a file.\n",
    "        new_words = [x.lower() for x in new_words]\n",
    "        \n",
    "        for f in range(0,len(data_frame_features)):            # iterating over each feature.\n",
    "            \n",
    "            count_w=0                                          # count_w will store count of f'th feature in that file.\n",
    "            if(data_frame_features[f]=='target_values_new'):\n",
    "                continue;\n",
    "            elif data_frame_features[f] not in new_words:      # if feature is not present in file continue(as np array is init as 0).\n",
    "                continue;\n",
    "            for w in new_words:\n",
    "                if w == data_frame_features[f]:\n",
    "                    count_w+=1;\n",
    "            cnt_array[f][0] = count_w                          # stores count of that feature\n",
    "\n",
    "        df_new.loc[k1] = [cnt_array[i][0] for i in range(cnt_array.shape[0])]      # appending that np.array as a row in our data_frame.\n",
    "        df_new.loc[k1, 'target_values_new'] = new_name          # adding target value for that particular row.\n",
    "        k1+=1\n",
    "\n",
    "print(\"DataFrame Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>any</th>\n",
       "      <th>some</th>\n",
       "      <th>who</th>\n",
       "      <th>no</th>\n",
       "      <th>so</th>\n",
       "      <th>which</th>\n",
       "      <th>when</th>\n",
       "      <th>don't</th>\n",
       "      <th>were</th>\n",
       "      <th>...</th>\n",
       "      <th>island,</th>\n",
       "      <th>cover,</th>\n",
       "      <th>houses.</th>\n",
       "      <th>tradition,</th>\n",
       "      <th>interprets</th>\n",
       "      <th>jul</th>\n",
       "      <th>hash</th>\n",
       "      <th>masks</th>\n",
       "      <th>urban</th>\n",
       "      <th>target_values_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6986 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   article   any  some   who    no    so  which  when  don't  were  \\\n",
       "0      0.0   2.0   3.0   3.0   0.0   3.0    8.0   2.0    0.0   1.0   \n",
       "1      3.0  15.0  31.0  23.0  22.0  25.0   14.0  12.0   15.0   4.0   \n",
       "2      1.0   2.0   2.0   0.0   2.0   1.0    1.0   1.0    3.0   1.0   \n",
       "3      0.0   1.0   1.0   0.0   0.0   1.0    0.0   0.0    0.0   0.0   \n",
       "4      1.0   1.0   0.0   0.0   0.0   0.0    0.0   0.0    0.0   0.0   \n",
       "\n",
       "         ...          island,  cover,  houses.  tradition,  interprets  jul  \\\n",
       "0        ...              0.0     0.0      0.0         0.0         0.0  0.0   \n",
       "1        ...              0.0     0.0      0.0         0.0         0.0  0.0   \n",
       "2        ...              0.0     0.0      0.0         0.0         0.0  0.0   \n",
       "3        ...              0.0     0.0      0.0         0.0         0.0  0.0   \n",
       "4        ...              0.0     0.0      0.0         0.0         0.0  0.0   \n",
       "\n",
       "   hash  masks  urban  target_values_new  \n",
       "0   0.0    0.0    0.0        alt.atheism  \n",
       "1   0.0    0.0    0.0        alt.atheism  \n",
       "2   0.0    0.0    0.0        alt.atheism  \n",
       "3   0.0    0.0    0.0        alt.atheism  \n",
       "4   0.0    0.0    0.0        alt.atheism  \n",
       "\n",
       "[5 rows x 6986 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(5)\n",
    "# About DataFrame:\n",
    "# Each row represents a particular email and the last col in each row represents the class it belongs to.\n",
    "# Each col is a word(top frequency word) from the whole dataset.\n",
    "# DF(i, j) represents the count of j'th word in i'th email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6985\n"
     ]
    }
   ],
   "source": [
    "# re-initialising 'data_frame_features' to new_features.\n",
    "while 'target_values_new' in new_features:\n",
    "    new_features.remove('target_values_new')\n",
    "data_frame_features = new_features\n",
    "print(len(data_frame_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc \n",
      "Test DataFrame Created.\n"
     ]
    }
   ],
   "source": [
    "                                            # DATA_FRAME creation(for testing file.)\n",
    "# creating data-frame.\n",
    "k1=0        # init k1.\n",
    "data_frame_features.append('target_values_new')             # appending exttra column in features to store the output.\n",
    "test_df = pd.DataFrame(columns = data_frame_features)        # creating an empty dataframe with columns as features and one extra column as output.\n",
    "\n",
    "for new_name in target:                       # iterating over each folder:\n",
    "    print(new_name,end=\" \")\n",
    "    \n",
    "    path = \"C:\\\\Users\\\\Archit\\\\Desktop\\\\new\\\\20_newsgroups_test\\\\\" + new_name    # path of testing_file.\n",
    "    \n",
    "    for new_file in glob.glob(os.path.join(path, '*')):     # iterating over each file in that particular folder.\n",
    "        cnt_array = np.zeros((len(data_frame_features),1))   # creating an np array to store count of each 'feature' in each file.\n",
    "        file_n = open(new_file,'r').read()\n",
    "        new_words = file_n.split()                           # 'new_words' is a list that stores each word of a file.\n",
    "        new_words = [x.lower() for x in new_words]\n",
    "        \n",
    "        for f in range(0,len(data_frame_features)):            # iterating over each feature.\n",
    "            count_w=0                                          # 'count_w' will store count of f'th feature in that file.\n",
    "            if(data_frame_features[f]=='target_values_new'):\n",
    "                continue;\n",
    "            elif data_frame_features[f] not in new_words:      # if feature is not present in file continue(as np array is init as 0).\n",
    "                continue;\n",
    "            for w in new_words:\n",
    "                if w == data_frame_features[f]:                # calculating occur. of that part. feature in test_file.\n",
    "                    count_w+=1;\n",
    "            cnt_array[f][0] = count_w                          # stores count of that feature\n",
    "\n",
    "        test_df.loc[k1] = [cnt_array[i][0] for i in range(cnt_array.shape[0])]      # appending that np.array as a row in our data_frame.\n",
    "        test_df.loc[k1, 'target_values_new'] = new_name                    # adding target value for that particular row.\n",
    "        k1+=1\n",
    "\n",
    "print()\n",
    "print(\"Test DataFrame Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15997, 6985), (15997,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting training_data into input and output.\n",
    "x_train = df_new.values[:, :-1]\n",
    "y_train = df_new.values[:, -1]\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 6985), (4000,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting testing_data into i/o and o/p.\n",
    "x_test = test_df.values[:, :-1]\n",
    "y_test = test_df.values[:, -1]\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "41.39916014671326\n"
     ]
    }
   ],
   "source": [
    "# clf is the classifier of MultiNomialNB(MNB).\n",
    "st = time.time()\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "et = time.time()\n",
    "y_pred = clf.predict(x_test)\n",
    "print(y_pred.shape)\n",
    "time_taken = et-st\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79925"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test,y_test) #nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.64      0.77      0.70       200\n",
      "           comp.graphics       0.69      0.79      0.74       200\n",
      " comp.os.ms-windows.misc       0.64      0.72      0.68       200\n",
      "comp.sys.ibm.pc.hardware       0.75      0.74      0.74       200\n",
      "   comp.sys.mac.hardware       0.80      0.81      0.80       200\n",
      "          comp.windows.x       0.88      0.64      0.74       200\n",
      "            misc.forsale       0.78      0.89      0.83       200\n",
      "               rec.autos       0.85      0.95      0.90       200\n",
      "         rec.motorcycles       0.84      0.94      0.89       200\n",
      "      rec.sport.baseball       0.96      0.97      0.96       200\n",
      "        rec.sport.hockey       0.96      0.95      0.96       200\n",
      "               sci.crypt       0.92      0.88      0.90       200\n",
      "         sci.electronics       0.80      0.78      0.79       200\n",
      "                 sci.med       0.89      0.82      0.86       200\n",
      "               sci.space       0.87      0.88      0.88       200\n",
      "  soc.religion.christian       0.84      0.88      0.86       200\n",
      "      talk.politics.guns       0.85      0.85      0.85       200\n",
      "   talk.politics.mideast       0.87      0.83      0.85       200\n",
      "      talk.politics.misc       0.61      0.59      0.60       200\n",
      "      talk.religion.misc       0.51      0.30      0.38       200\n",
      "\n",
      "             avg / total       0.80      0.80      0.79      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153   1   1   0   0   0   1   0   2   0   0   0   1   2   1   3   1   2\n",
      "    0  32]\n",
      " [  2 158   8   5   7   4   1   0   1   0   1   3   3   2   0   1   0   0\n",
      "    0   4]\n",
      " [  0  14 144  14   7   8   1   1   2   1   0   1   2   0   0   0   1   0\n",
      "    2   2]\n",
      " [  0   4  13 148  14   1  10   1   0   0   0   0   8   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   8  13 162   0   8   2   0   0   0   1   2   0   2   0   0   0\n",
      "    0   0]\n",
      " [  0  22  40   2   1 127   3   0   2   0   1   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0  10   1   0 178   2   2   1   0   0   3   0   0   1   0   0\n",
      "    1   0]\n",
      " [  0   1   1   0   0   0   4 190   0   0   0   1   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   4   4 188   0   0   0   2   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   2   1   0 194   2   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  1   0   0   0   0   0   2   0   1   1 191   0   0   1   0   1   0   0\n",
      "    2   0]\n",
      " [  0   6   2   1   3   1   1   1   1   0   0 176   4   0   1   0   2   0\n",
      "    1   0]\n",
      " [  0   9   3   4   6   0   5   4   6   0   0   3 156   1   2   0   0   0\n",
      "    0   1]\n",
      " [  4   2   0   0   0   0   1   5   9   0   2   0   5 165   1   0   1   0\n",
      "    3   2]\n",
      " [  2   4   0   0   0   0   1   4   1   0   0   0   3   2 176   0   0   0\n",
      "    3   4]\n",
      " [  5   3   3   0   0   1   1   0   1   1   0   0   1   2   0 176   0   0\n",
      "    1   5]\n",
      " [  0   0   0   1   0   0   0   2   0   1   0   2   1   1   3   0 170   1\n",
      "   17   1]\n",
      " [  5   0   0   0   0   1   1   1   2   0   0   1   1   0   3   1   4 166\n",
      "   12   2]\n",
      " [  5   0   2   0   1   0   3   4   2   3   1   2   0   7   8   4  17  18\n",
      "  119   4]\n",
      " [ 63   0   0   0   1   1   0   1   4   1   0   1   1   2   4  23   2   3\n",
      "   33  60]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
